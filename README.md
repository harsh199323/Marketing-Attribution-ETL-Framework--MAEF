---

# **Airflow ETL Pipeline** ğŸš€

ğŸ“Š **Automated ETL pipeline using Apache Airflow for data extraction, transformation, and reporting.**

![Airflow Logo](https://upload.wikimedia.org/wikipedia/commons/d/de/AirflowLogo.png)  

---

## ğŸ“Œ **Table of Contents**

- [ğŸ”¹ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸš€ Usage](#-usage)
- [ğŸ”„ Workflow](#-workflow)
- [ğŸ› ï¸ Customization](#ï¸-customization)
- [ğŸ“ˆ Logging & Monitoring](#-logging--monitoring)
- [ğŸš€ Future Enhancements](#-future-enhancements)
- [ğŸ“œ License](#-license)
- [ğŸ‘¤ Author](#-author)

---

## ğŸ”¹ **Features**

âœ” **Automated DAG Execution** via Apache Airflow  
âœ” **Database Setup** and schema verification  
âœ” **Data Transformation & Processing**  
âœ” **API Integration** for external data transmission  
âœ” **Error Handling & Status Updates**  
âœ” **Attribution & Reporting** in CSV format  

---

## ğŸ—ï¸ **Architecture**

The ETL pipeline follows a modular architecture, divided into **distinct components**:

```mermaid
graph TD;
    A[Trigger DAG] -->|Start| B[Setup Database];
    B --> C[Load Raw Data];
    C --> D[Transform Data];
    D --> E[Save Processed Data];
    E -->|Send to API| F[API Response Handling];
    F --> G[Store in Database];
    G --> H[Generate Reports];
    H --> I[Export CSV];
```

ğŸ“Œ **Main Components:**  
ğŸ”¹ **Airflow DAG** â†’ Manages task execution  
ğŸ”¹ **DataWarehouse** â†’ Stores raw & processed data  
ğŸ”¹ **DataTransformer** â†’ Processes and transforms data  
ğŸ”¹ **IHCApiClient** â†’ Sends data to API  
ğŸ”¹ **DataLoader** â†’ Loads data into warehouse  
ğŸ”¹ **ChannelReporting** â†’ Aggregates reporting metrics  
ğŸ”¹ **SQLite DB** â†’ Stores final transformed data  

---

## âš™ï¸ **Installation**

### **Prerequisites**
âœ” Python 3.x  
âœ” Apache Airflow  
âœ” SQLite (or PostgreSQL, MySQL)  
âœ” Required Python libraries (see `requirements.txt`)  

## **Databse Files**
To get the necessary database files, download them from the original source:
[Recruitment Challenge - Data Engineering](https://github.com/haensel-ams/recruitment_challenge/tree/master/Data_Engineering_202309).


### **Setup Instructions**
```sh
# Clone repository
git clone https://github.com/yourusername/airflow-etl-pipeline.git
cd airflow-etl-pipeline

# Install dependencies
pip install -r requirements.txt

# Initialize Airflow
airflow db init
airflow users create --username admin --firstname John --lastname Doe --role Admin --email admin@example.com

# Start Airflow services
airflow scheduler &
airflow webserver --port 8080
```
ğŸ“Œ **Access Airflow UI:** `http://localhost:8080`

---

## ğŸš€ **Usage**

1. **Trigger DAG** manually or schedule periodic execution.  
2. **Monitor Airflow UI** for DAG execution progress.  
3. **Check SQLite Database** for processed data.  
4. **Export & Analyze CSV Reports** generated in the pipeline.  

---

## ğŸ”„ **Workflow Overview**

```mermaid
sequenceDiagram
    participant UI as Airflow UI
    participant DAG as Airflow DAG
    participant DW as DataWarehouse
    participant DT as DataTransformer
    participant API as IHCApiClient
    participant DB as SQLite DB
    participant CR as ChannelReporting

    UI->>DAG: Trigger DAG
    DAG->>DW: setup_database()
    DAG->>DT: transform_data(start_date, end_date)
    DT->>API: send_data_to_api()
    API-->>DB: Save API Responses
    API-->>DAG: Return Error Status (if any)
    DAG->>CR: create_channel_report()
    CR->>DB: Aggregate Metrics
    CR->>UI: Export CSV Report
```

---

## ğŸ› ï¸ **Customization**

ğŸ“Œ **Modify ETL Logic:**  
ğŸ”¹ Adjust transformations in `data_transformer.py`  
ğŸ”¹ Change API configurations in `ihc_api_client.py`  
ğŸ”¹ Update report structure in `channel_reporting.py`  
ğŸ”¹ Edit DAG schedules in `dags/etl_pipeline.py`  

---

## ğŸ“ˆ **Logging & Monitoring**

ğŸ” **Airflow Logs:** Located in `~/airflow/logs/`  
ğŸ› ï¸ **Debugging Commands:**
```sh
airflow tasks logs <dag_id> <task_id>
```
ğŸ“Š **Monitor API Requests:**  
ğŸ”¹ Logs saved in `ihc_api_client.py`  

---

## ğŸš€ **Future Enhancements**

âœ¨ **Upgrade to PostgreSQL or MySQL**  
âœ¨ **Enhance Data Validation & Integrity Checks**  
âœ¨ **Implement Real-Time Monitoring Dashboards**  
âœ¨ **Optimize API Integration for Scalability**  

